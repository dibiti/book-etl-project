# .gitignore
__pycache__/
*.pyc
data/*.csv

---

# requirements.txt
pandas
requests
beautifulsoup4

---

# README.md
# Book ETL Project - Books to Scrape

This project implements an ETL pipeline in Python that collects book data from the website [Books to Scrape](http://books.toscrape.com/), transforms the data, and saves it into a CSV file.

## Project Structure

```
book-etl-project/
├── etl/
│   ├── extract.py       # Extracts data from the website
│   ├── transform.py     # Cleans and standardizes the data
│   └── load.py          # Saves the data into a CSV file
├── data/
│   └── books.csv        # File generated by the ETL process
├── main.py              # Entry point to run the ETL pipeline
├── requirements.txt     # Required libraries
├── README.md            # Project description and instructions
└── .gitignore           # Ignored files and folders
```

## How to Use

1. Clone this repository:
```bash
git clone https://github.com/your-username/book-etl-project.git
cd book-etl-project
```

2. Install the dependencies:
```bash
pip install -r requirements.txt
```

3. Run the pipeline:
```bash
python main.py
```

The `books.csv` file will be generated in the `/data` folder.

---

# main.py
from etl.extract import extract_books
from etl.transform import transform_books
from etl.load import load_books

def run_etl():
    print("Starting ETL pipeline...")
    books = extract_books()
    books_cleaned = transform_books(books)
    load_books(books_cleaned)
    print("ETL completed successfully. File saved to data/books.csv")

if __name__ == "__main__":
    run_etl()

---

# etl/extract.py
import requests
from bs4 import BeautifulSoup
import pandas as pd

def extract_books():
    """
    Extract book data from all pages of the website.

    Returns:
        pd.DataFrame: DataFrame containing book information.
    """
    base_url = "http://books.toscrape.com/catalogue/page-{}.html"
    books = []

    for page in range(1, 51):  # 50 pages, each with 20 books
        print(f"Extracting page {page}...")
        url = base_url.format(page)
        response = requests.get(url)
        if response.status_code != 200:
            print(f"Failed to retrieve page {page}. Status code: {response.status_code}")
            break

        soup = BeautifulSoup(response.text, 'html.parser')
        articles = soup.select('article.product_pod')

        for article in articles:
            title = article.h3.a['title']
            price = article.select_one('.price_color').text
            stock = article.select_one('.availability').text.strip()
            rating = article.select_one('p')['class'][1]  # e.g., 'Three'
            link = article.h3.a['href']

            books.append({
                'title': title,
                'price': price,
                'stock': stock,
                'rating': rating,
                'url': f"http://books.toscrape.com/catalogue/{link}"
            })

    return pd.DataFrame(books)

---

# etl/transform.py
import pandas as pd

def transform_books(df):
    """
    Clean and transform book data.

    Args:
        df (pd.DataFrame): Raw book data.

    Returns:
        pd.DataFrame: Cleaned book data.
    """
    df = df.copy()

    # Remove the currency symbol and convert to float
    df['price'] = df['price'].str.replace('£', '').astype(float)

    # Extract number of books in stock
    df['stock'] = df['stock'].str.extract(r'(\d+)').astype(int)

    # Map textual rating to numeric values
    rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}
    df['rating'] = df['rating'].map(rating_map)

    return df

---

# etl/load.py
import os

def load_books(df):
    """
    Save the cleaned book data to a CSV file.

    Args:
        df (pd.DataFrame): Cleaned book data.
    """
    os.makedirs('data', exist_ok=True)
    df.to_csv('data/books.csv', index=False)
